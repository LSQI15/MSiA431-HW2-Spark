{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.functions import *\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.ml import Pipeline\n",
    "import pyspark.sql.functions as func\n",
    "from pyspark.sql.types import TimestampType\n",
    "from datetime import datetime\n",
    "\n",
    "sc = SparkContext()\n",
    "sqlcontext = SQLContext(sc)\n",
    "path = \"hdfs://wolf.analytics.private/user/slx4192/data/crime/Crimes_-_2001_to_present.csv\"\n",
    "mydata = sqlcontext.read.csv(path, header=True).sample(.01, False, 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "Predict the number of crime events in the next week at the beat level. Violent crime events represent a greater threat to the public and thus it is desirable that they are forecasted more accurately (IUCR codes available here: https://data.cityofchicago.org/widgets/c7ck-438e). (45 pts) You are encouraged to bring in additional data sets. (extra 10 pts if you mix the existing data with an exogenous data set) Report the accuracy of your models. You must use Spark dataframes and ML pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ID: string, Case Number: string, Date: string, Block: string, IUCR: string, Primary Type: string, Description: string, Location Description: string, Arrest: string, Domestic: string, Beat: string, District: string, Ward: string, Community Area: string, FBI Code: string, X Coordinate: string, Y Coordinate: string, Year: string, Updated On: string, Latitude: string, Longitude: string, Location: string, Date_time: timestamp, Week_num: int, Violent: int]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getDateTime = udf(lambda x: datetime.strptime( x, '%m/%d/%Y %I:%M:%S %p'), TimestampType())\n",
    "mydata_violent = mydata\\\n",
    "                    .withColumn('Date_time', getDateTime(col('Date')))\\\n",
    "                    .withColumn('Week_num', weekofyear('Date_time'))\\\n",
    "                    .withColumn(\"Violent\",func.when(mydata[\"IUCR\"].like(\"01%\") | mydata[\"IUCR\"].like(\"02%\") |\\\n",
    "                                  mydata[\"IUCR\"].like(\"03%\") | mydata[\"IUCR\"].like(\"04%\") |\\\n",
    "                                  mydata[\"IUCR\"].like(\"05%\") | mydata[\"IUCR\"].like(\"06%\") |\\\n",
    "                                  mydata[\"IUCR\"].like(\"10%\") | mydata[\"IUCR\"].like(\"13%\") |\\\n",
    "                                  mydata[\"IUCR\"].like(\"24%\") | mydata[\"IUCR\"].like(\"39%\") |\\\n",
    "                                  mydata[\"IUCR\"].like(\"42%\"),1).otherwise(0))\n",
    "mydata_violent.persist()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------------+\n",
      "|Beat|count|Year_WeekNum|\n",
      "+----+-----+------------+\n",
      "|0111|    1|     2001-01|\n",
      "|0111|    1|     2001-11|\n",
      "|0111|    2|     2001-29|\n",
      "|0111|    1|     2001-37|\n",
      "|0111|    1|     2001-39|\n",
      "|0111|    1|     2001-43|\n",
      "|0111|    1|     2001-48|\n",
      "|0111|    3|     2001-50|\n",
      "|0111|    1|     2002-01|\n",
      "|0111|    1|     2002-23|\n",
      "|0111|    1|     2002-40|\n",
      "|0111|    1|     2002-41|\n",
      "|0111|    1|     2002-42|\n",
      "|0111|    1|     2002-43|\n",
      "|0111|    1|     2002-44|\n",
      "|0111|    1|     2002-47|\n",
      "|0111|    2|     2002-52|\n",
      "|0111|    1|     2003-04|\n",
      "|0111|    1|     2003-08|\n",
      "|0111|    2|     2003-17|\n",
      "+----+-----+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "agg_crime = mydata_violent.groupBy(\"Beat\", \"Year\", \"Week_num\", \"Violent\").count()\n",
    "\n",
    "nv_crime = mydata_violent.groupBy(\"Year\",\"Beat\",\"Week_num\").count().orderBy(\"Beat\", \"Year\", \"Week_num\")\\\n",
    "                .withColumn(\"Year_WeekNum\", concat(mydata_violent.Year, lpad(mydata_violent.Week_num, 3, \"-0\")))\\\n",
    "                .drop(\"Year\", \"Week_num\")\n",
    "nv_crime.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------------+----+----+----+----+----+----+----+----+\n",
      "|Beat|count|Year_WeekNum|lag1|lag2|lag3|lag4|lag5|lag6|lag7|lag8|\n",
      "+----+-----+------------+----+----+----+----+----+----+----+----+\n",
      "|0111|    1|     2001-01|null|null|null|null|null|null|null|null|\n",
      "|0111|    1|     2001-11|   1|null|null|null|null|null|null|null|\n",
      "|0111|    2|     2001-29|   1|   1|null|null|null|null|null|null|\n",
      "|0111|    1|     2001-37|   2|   1|   1|null|null|null|null|null|\n",
      "|0111|    1|     2001-39|   1|   2|   1|   1|null|null|null|null|\n",
      "|0111|    1|     2001-43|   1|   1|   2|   1|   1|null|null|null|\n",
      "|0111|    1|     2001-48|   1|   1|   1|   2|   1|   1|null|null|\n",
      "|0111|    3|     2001-50|   1|   1|   1|   1|   2|   1|   1|null|\n",
      "|0111|    1|     2002-01|   3|   1|   1|   1|   1|   2|   1|   1|\n",
      "|0111|    1|     2002-23|   1|   3|   1|   1|   1|   1|   2|   1|\n",
      "+----+-----+------------+----+----+----+----+----+----+----+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "nv_crime_w_lag = nv_crime\\\n",
    "        .withColumn('lag1', lag('count').over(Window.partitionBy(\"Beat\").orderBy(\"Year_WeekNum\")))\\\n",
    "        .withColumn('lag2', lag('count',2).over(Window.partitionBy(\"Beat\").orderBy(\"Year_WeekNum\")))\\\n",
    "        .withColumn('lag3', lag('count',3).over(Window.partitionBy(\"Beat\").orderBy(\"Year_WeekNum\")))\\\n",
    "        .withColumn('lag4', lag('count',4).over(Window.partitionBy(\"Beat\").orderBy(\"Year_WeekNum\")))\\\n",
    "        .withColumn('lag5', lag('count',5).over(Window.partitionBy(\"Beat\").orderBy(\"Year_WeekNum\")))\\\n",
    "        .withColumn('lag6', lag('count',6).over(Window.partitionBy(\"Beat\").orderBy(\"Year_WeekNum\")))\\\n",
    "        .withColumn('lag7', lag('count',7).over(Window.partitionBy(\"Beat\").orderBy(\"Year_WeekNum\")))\\\n",
    "        .withColumn('lag8', lag('count',8).over(Window.partitionBy(\"Beat\").orderBy(\"Year_WeekNum\")))\\\n",
    "        .orderBy(\"Beat\",\"Year_WeekNum\")\n",
    "nv_crime_w_lag.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----+----+----+----+----+----+----+----+----+-------+\n",
      "|Beat|count|lag1|lag2|lag3|lag4|lag5|lag6|lag7|lag8|Year|WeekNum|\n",
      "+----+-----+----+----+----+----+----+----+----+----+----+-------+\n",
      "|0111|    1|   3|   1|   1|   1|   1|   2|   1|   1|2002|      1|\n",
      "|0111|    1|   1|   3|   1|   1|   1|   1|   2|   1|2002|     23|\n",
      "|0111|    1|   1|   1|   3|   1|   1|   1|   1|   2|2002|     40|\n",
      "|0111|    1|   1|   1|   1|   3|   1|   1|   1|   1|2002|     41|\n",
      "|0111|    1|   1|   1|   1|   1|   3|   1|   1|   1|2002|     42|\n",
      "|0111|    1|   1|   1|   1|   1|   1|   3|   1|   1|2002|     43|\n",
      "|0111|    1|   1|   1|   1|   1|   1|   1|   3|   1|2002|     44|\n",
      "|0111|    1|   1|   1|   1|   1|   1|   1|   1|   3|2002|     47|\n",
      "|0111|    2|   1|   1|   1|   1|   1|   1|   1|   1|2002|     52|\n",
      "|0111|    1|   2|   1|   1|   1|   1|   1|   1|   1|2003|      4|\n",
      "+----+-----+----+----+----+----+----+----+----+----+----+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType,DoubleType\n",
    "nv_crime_final = nv_crime_w_lag\\\n",
    "    .withColumn(\"Year\", nv_crime_w_lag[\"Year_WeekNum\"].substr(0,4).cast(IntegerType()))\\\n",
    "    .withColumn(\"WeekNum\", nv_crime_w_lag[\"Year_WeekNum\"].substr(6,2).cast(IntegerType()))\\\n",
    "    .drop(\"Year_WeekNum\").na.drop()\n",
    "nv_crime_final.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler,StringIndexer,OneHotEncoderEstimator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "\n",
    "tmp = str(nv_crime_final.select(countDistinct(\"Beat\")).collect()[0])\n",
    "dis_beats = int(tmp[tmp.find('=')+1:-1])\n",
    "\n",
    "BeatIdx = StringIndexer(inputCol='Beat',outputCol='BeatIdx')\n",
    "WeekNumIdx = StringIndexer(inputCol='WeekNum',outputCol='WeekNumIdx')\n",
    "encoder = OneHotEncoderEstimator(inputCols = [\"BeatIdx\",\"WeekNumIdx\"], outputCols = [\"BeatVec\",\"WeekNumVec\"]).setHandleInvalid(\"keep\")\n",
    "\n",
    "assembler = VectorAssembler(inputCols=[\"BeatVec\",\"Year\",\"WeekNumVec\",\"lag1\", \"lag2\", \"lag3\", \"lag4\", \"lag5\", \"lag6\", \"lag7\", \"lag8\"], outputCol='features')\n",
    "gradient_boosted = GBTRegressor(labelCol=\"count\", featuresCol=\"features\",maxBins = dis_beats, maxIter=10)\n",
    "\n",
    "pipeline = Pipeline(stages = [BeatIdx, WeekNumIdx, encoder, assembler, gradient_boosted])\n",
    "train, test = nv_crime_final.randomSplit([0.8, 0.2])\n",
    "\n",
    "model = pipeline.fit(train)\n",
    "predictions = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1726275643852226\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "predictions2 = predictions.select(col(\"count\").cast(\"Float\"), col(\"prediction\"))\n",
    "evaluator_mse = RegressionEvaluator(labelCol=\"count\", predictionCol=\"prediction\", metricName=\"mse\")\n",
    "mse = evaluator_mse.evaluate(predictions2)\n",
    "\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
