{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.functions import *\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.ml import Pipeline\n",
    "import pyspark.sql.functions as func\n",
    "from pyspark.sql.types import TimestampType\n",
    "from datetime import datetime\n",
    "\n",
    "sc = SparkContext()\n",
    "sqlcontext = SQLContext(sc)\n",
    "path = \"hdfs://wolf.analytics.private/user/slx4192/data/crime/Crimes_-_2001_to_present.csv\"\n",
    "mydata = sqlcontext.read.csv(path, header=True).sample(.01, False, 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "Predict the number of crime events in the next week at the beat level. Violent crime events represent a greater threat to the public and thus it is desirable that they are forecasted more accurately (IUCR codes available here: https://data.cityofchicago.org/widgets/c7ck-438e). (45 pts) You are encouraged to bring in additional data sets. (extra 10 pts if you mix the existing data with an exogenous data set) Report the accuracy of your models. You must use Spark dataframes and ML pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ID: string, Case Number: string, Date: string, Block: string, IUCR: string, Primary Type: string, Description: string, Location Description: string, Arrest: string, Domestic: string, Beat: string, District: string, Ward: string, Community Area: string, FBI Code: string, X Coordinate: string, Y Coordinate: string, Year: string, Updated On: string, Latitude: string, Longitude: string, Location: string, Date_time: timestamp, Week_num: int, Violent: int]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getDateTime = udf(lambda x: datetime.strptime( x, '%m/%d/%Y %I:%M:%S %p'), TimestampType())\n",
    "mydata_violent = mydata\\\n",
    "                    .withColumn('Date_time', getDateTime(col('Date')))\\\n",
    "                    .withColumn('Week_num', weekofyear('Date_time'))\\\n",
    "                    .withColumn(\"Violent\",func.when(mydata[\"IUCR\"].like(\"01%\") | mydata[\"IUCR\"].like(\"02%\") |\\\n",
    "                                  mydata[\"IUCR\"].like(\"03%\") | mydata[\"IUCR\"].like(\"04%\") |\\\n",
    "                                  mydata[\"IUCR\"].like(\"05%\") | mydata[\"IUCR\"].like(\"06%\") |\\\n",
    "                                  mydata[\"IUCR\"].like(\"10%\") | mydata[\"IUCR\"].like(\"13%\") |\\\n",
    "                                  mydata[\"IUCR\"].like(\"24%\") | mydata[\"IUCR\"].like(\"39%\") |\\\n",
    "                                  mydata[\"IUCR\"].like(\"42%\"),1).otherwise(0))\n",
    "mydata_violent.persist()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_crime = mydata_violent.groupBy(\"Beat\", \"Year\", \"Week_num\", \"Violent\").count()\n",
    "v_crime = agg_crime.filter(agg_crime.Violent == 1)\\\n",
    "    .drop(\"Violent\").orderBy(\"Beat\", \"Year\", \"Week_num\")\\\n",
    "    .withColumn(\"Year_WeekNum\", concat(agg_crime.Year, lpad(agg_crime.Week_num, 3, \"-0\")))\\\n",
    "    .drop(\"Year\", \"Week_num\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------------+----+----+----+----+----+----+----+----+\n",
      "|Beat|count|Year_WeekNum|lag1|lag2|lag3|lag4|lag5|lag6|lag7|lag8|\n",
      "+----+-----+------------+----+----+----+----+----+----+----+----+\n",
      "|0111|    1|     2001-43|null|null|null|null|null|null|null|null|\n",
      "|0111|    1|     2001-48|   1|null|null|null|null|null|null|null|\n",
      "|0111|    1|     2002-40|   1|   1|null|null|null|null|null|null|\n",
      "|0111|    1|     2003-31|   1|   1|   1|null|null|null|null|null|\n",
      "|0111|    1|     2003-43|   1|   1|   1|   1|null|null|null|null|\n",
      "|0111|    1|     2004-06|   1|   1|   1|   1|   1|null|null|null|\n",
      "|0111|    2|     2005-29|   1|   1|   1|   1|   1|   1|null|null|\n",
      "|0111|    1|     2005-39|   2|   1|   1|   1|   1|   1|   1|null|\n",
      "|0111|    1|     2006-34|   1|   2|   1|   1|   1|   1|   1|   1|\n",
      "|0111|    1|     2006-44|   1|   1|   2|   1|   1|   1|   1|   1|\n",
      "+----+-----+------------+----+----+----+----+----+----+----+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "v_crime_w_lag = v_crime\\\n",
    "        .withColumn('lag1', lag('count').over(Window.partitionBy(\"Beat\").orderBy(\"Year_WeekNum\")))\\\n",
    "        .withColumn('lag2', lag('count',2).over(Window.partitionBy(\"Beat\").orderBy(\"Year_WeekNum\")))\\\n",
    "        .withColumn('lag3', lag('count',3).over(Window.partitionBy(\"Beat\").orderBy(\"Year_WeekNum\")))\\\n",
    "        .withColumn('lag4', lag('count',4).over(Window.partitionBy(\"Beat\").orderBy(\"Year_WeekNum\")))\\\n",
    "        .withColumn('lag5', lag('count',5).over(Window.partitionBy(\"Beat\").orderBy(\"Year_WeekNum\")))\\\n",
    "        .withColumn('lag6', lag('count',6).over(Window.partitionBy(\"Beat\").orderBy(\"Year_WeekNum\")))\\\n",
    "        .withColumn('lag7', lag('count',7).over(Window.partitionBy(\"Beat\").orderBy(\"Year_WeekNum\")))\\\n",
    "        .withColumn('lag8', lag('count',8).over(Window.partitionBy(\"Beat\").orderBy(\"Year_WeekNum\")))\\\n",
    "        .orderBy(\"Beat\",\"Year_WeekNum\")\n",
    "v_crime_w_lag.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----+----+----+----+----+----+----+----+----+-------+\n",
      "|Beat|count|lag1|lag2|lag3|lag4|lag5|lag6|lag7|lag8|Year|WeekNum|\n",
      "+----+-----+----+----+----+----+----+----+----+----+----+-------+\n",
      "|0111|    1|   1|   2|   1|   1|   1|   1|   1|   1|2006|     34|\n",
      "|0111|    1|   1|   1|   2|   1|   1|   1|   1|   1|2006|     44|\n",
      "|0111|    1|   1|   1|   1|   2|   1|   1|   1|   1|2006|     49|\n",
      "|0111|    1|   1|   1|   1|   1|   2|   1|   1|   1|2006|     51|\n",
      "|0111|    1|   1|   1|   1|   1|   1|   2|   1|   1|2007|     24|\n",
      "|0111|    1|   1|   1|   1|   1|   1|   1|   2|   1|2007|     28|\n",
      "|0111|    1|   1|   1|   1|   1|   1|   1|   1|   2|2007|     33|\n",
      "|0111|    1|   1|   1|   1|   1|   1|   1|   1|   1|2007|     46|\n",
      "|0111|    1|   1|   1|   1|   1|   1|   1|   1|   1|2007|     49|\n",
      "|0111|    1|   1|   1|   1|   1|   1|   1|   1|   1|2008|     35|\n",
      "+----+-----+----+----+----+----+----+----+----+----+----+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType,DoubleType\n",
    "v_crime_final = v_crime_w_lag\\\n",
    "    .withColumn(\"Year\", v_crime_w_lag[\"Year_WeekNum\"].substr(0,4).cast(IntegerType()))\\\n",
    "    .withColumn(\"WeekNum\", v_crime_w_lag[\"Year_WeekNum\"].substr(6,2).cast(IntegerType()))\\\n",
    "    .drop(\"Year_WeekNum\").na.drop()\n",
    "v_crime_final.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler,StringIndexer,OneHotEncoderEstimator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "\n",
    "tmp = str(v_crime_final.select(countDistinct(\"Beat\")).collect()[0])\n",
    "dis_beats = int(tmp[tmp.find('=')+1:-1])\n",
    "\n",
    "BeatIdx = StringIndexer(inputCol='Beat',outputCol='BeatIdx')\n",
    "WeekNumIdx = StringIndexer(inputCol='WeekNum',outputCol='WeekNumIdx')\n",
    "encoder = OneHotEncoderEstimator(inputCols = [\"BeatIdx\",\"WeekNumIdx\"], outputCols = [\"BeatVec\",\"WeekNumVec\"]).setHandleInvalid(\"keep\")\n",
    "\n",
    "assembler = VectorAssembler(inputCols=[\"BeatVec\",\"Year\",\"WeekNumVec\",\"lag1\", \"lag2\", \"lag3\", \"lag4\", \"lag5\", \"lag6\", \"lag7\", \"lag8\"], outputCol='features')\n",
    "gradient_boosted = GBTRegressor(labelCol=\"count\", featuresCol=\"features\",maxBins = dis_beats, maxIter=10)\n",
    "\n",
    "pipeline = Pipeline(stages = [BeatIdx, WeekNumIdx, encoder, assembler, gradient_boosted])\n",
    "train, test = v_crime_final.randomSplit([0.8, 0.2])\n",
    "\n",
    "model = pipeline.fit(train)\n",
    "predictions = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08246821708894411\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "predictions2 = predictions.select(col(\"count\").cast(\"Float\"), col(\"prediction\"))\n",
    "evaluator_mse = RegressionEvaluator(labelCol=\"count\", predictionCol=\"prediction\", metricName=\"mse\")\n",
    "mse = evaluator_mse.evaluate(predictions2)\n",
    "\n",
    "print(mse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
